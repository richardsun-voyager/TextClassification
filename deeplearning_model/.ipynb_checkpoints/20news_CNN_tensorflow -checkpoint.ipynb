{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                  shuffle=True, random_state=11)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                  shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text number: 11314\n",
      "Testing text number: 7532\n"
     ]
    }
   ],
   "source": [
    "print('Training text number:', len(newsgroups_train.data))\n",
    "print('Testing text number:', len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class readNews:\n",
    "    '''\n",
    "    Read 20news and transform them into vectors for training\n",
    "    Args:\n",
    "    train_data\n",
    "    test_data\n",
    "    '''\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._preprocess()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('['+string.punctuation+']', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[^a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        '''Remove punctuations'''\n",
    "        train_news = self._train_data.data\n",
    "        test_news = self._test_data.data\n",
    "        self._train_data.data = [self._preProcessor(item) for item in train_news]\n",
    "        self._test_data.data = [self._preProcessor(item) for item in test_news]\n",
    "        \n",
    "    def _tfidf_vectorizer(self):\n",
    "        ''''Vectorize news'''\n",
    "        tfidfVectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_features=5000)\n",
    "        X_train_tfidf = tfidfVectorizer.fit_transform(self._train_data.data)\n",
    "        X_test_tfidf = tfidfVectorizer.transform(self._test_data.data)\n",
    "        vocab_index_dict = tfidfVectorizer.vocabulary_\n",
    "        return X_train_tfidf, X_test_tfidf, vocab_index_dict\n",
    "    \n",
    "    def tfidf_weight(self):\n",
    "        '''Calculate TfIdf weights for each word within each news'''\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        X_train_tfidf, X_test_tfidf, vocab_index_dict = self._tfidf_vectorizer()\n",
    "        train_weights = []\n",
    "        test_weights = []\n",
    "        #Generate dicts for words and corresponding tfidf weights\n",
    "        for i, news in enumerate(train_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_train_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            train_weights.append(word_weight)\n",
    "        for i, news in enumerate(test_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_test_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            test_weights.append(word_weight)      \n",
    "        return train_weights, test_weights\n",
    "    \n",
    "    def _news2words(self):\n",
    "        #Split each news into words\n",
    "        train_news_words = []\n",
    "        test_news_words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect words for each news\n",
    "           train_news_words.append(news.split())\n",
    "        for news in self._test_data.data:\n",
    "            test_news_words.append(news.split())\n",
    "        return train_news_words, test_news_words\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect all the chars\n",
    "           words.extend(news.split())\n",
    "        #Calculate frequencies of each character\n",
    "        word_freq = Counter(words)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in word_freq.items() if v>3]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return vocab, word_id_map, id_word_map\n",
    "    \n",
    "    def news2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        vocab, word_id_map, id_word_mapp = self.buildVocab()\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        #Turn each news into a list of word Ids\n",
    "        words_vecs = lambda words: [word2id(w) for w in words]\n",
    "        train_news_vecs = [words_vecs(words) for words in train_news_words]\n",
    "        train_news_labels = self._train_data.target\n",
    "        test_news_vecs = [words_vecs(words) for words in test_news_words]\n",
    "        test_news_labels = self._test_data.target\n",
    "        return train_news_vecs, train_news_labels, test_news_vecs, test_news_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a readnews object\n",
    "rn = readNews(newsgroups_train, newsgroups_test)\n",
    "train_news_vecs, train_news_labels, test_news_vecs, test_news_labels = rn.news2vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Record tfidf weights for each word in each news\n",
    "train_weights, test_weights = rn.tfidf_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the vocabulary and dictionary of words as well as corresponding ids\n",
    "vocab, word_id_map, id_word_map = rn.buildVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(c):\n",
    "    try:\n",
    "        ID = word_id_map[c]\n",
    "    except:#Trun those less frequent words into UNK\n",
    "        ID = word_id_map['UNK']\n",
    "    return ID\n",
    "def id2word(c):\n",
    "    try:\n",
    "        word = id_word_map[c]\n",
    "    except:\n",
    "        word='UNK'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Length 17\n",
      "Max Length 15804\n",
      "Median Length 184.0\n"
     ]
    }
   ],
   "source": [
    "train_news_length = [len(news) for news in train_news_vecs]\n",
    "print('Min Length', np.amin(train_news_length))\n",
    "print('Max Length', np.max(train_news_length))\n",
    "print('Median Length', np.median(train_news_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17.,  116.,  184.,  301.,  509.,  769.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_news_length, [0, 25, 50, 75, 90, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the length varies much, perhaps we need buckets to put news with similar lengths together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class generateSamples:\n",
    "    '''Generate samples for training and testing'''\n",
    "    \n",
    "    def __init__(self, news_vecs, news_labels, weights, max_len=800):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.index = 0\n",
    "        self.news_vecs = news_vecs\n",
    "        self.news_labels = news_labels\n",
    "        self.weights= weights\n",
    "        self.news_count = len(news_vecs)\n",
    "        self.max_news_len = max_len\n",
    "        \n",
    "    def generate_batch(self, batch_size=64, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        \n",
    "        selected_samples = []\n",
    "        selected_labels = []\n",
    "        batch_weights = []\n",
    "        #For training, select random samples\n",
    "        if is_training:\n",
    "            selected_index = np.random.choice(len(self.news_vecs), batch_size, replace=False)\n",
    "            for index in selected_index:\n",
    "                selected_samples.append(self.news_vecs[index])\n",
    "                selected_labels.append(self.news_labels[index])\n",
    "                batch_weights.append(self.weights[index])\n",
    "        #For testing, select a few samples each time\n",
    "        else:#Testing model\n",
    "            start = self.index%self.news_count\n",
    "            end = (start + batch_size)%self.news_count\n",
    "            #In case end goes beyong the range of the samples\n",
    "            if end > start:\n",
    "                selected_samples = self.news_vecs[start: end]\n",
    "                selected_labels = self.news_labels[start: end]\n",
    "                #record weights\n",
    "                batch_weights = self.weights[start:end]\n",
    "                self.index = end\n",
    "            else:\n",
    "                print('Test Samples come to an end!')\n",
    "                selected_samples = self.news_vecs[start: ]\n",
    "                #record weights\n",
    "                batch_weights = self.weights[start:]\n",
    "                selected_labels = self.news_labels[start: ]\n",
    "                self.index = 0\n",
    "            \n",
    "        #Set the max lengths as the size of the input\n",
    "        #max_len = max(map(len, data))\n",
    "        #Record lengths for each text\n",
    "        lengths = [len(item) for item in selected_samples]\n",
    "        lengths = np.array(lengths)\n",
    "        #Get the max length in current batch\n",
    "        max_len = self.max_news_len\n",
    "        #max_len = self.max_news_len if max_len > self.max_news_len else max_len\n",
    "\n",
    "        #Create input and label\n",
    "        x = np.full((batch_size, max_len), word2id('UNK'), np.int32)\n",
    "        w = np.full((batch_size, max_len), 0, np.float32)\n",
    "        y = np.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            #the first n elements as input\n",
    "            if len(selected_samples[i]) < max_len:\n",
    "                x[i, :len(selected_samples[i])] = selected_samples[i]\n",
    "                w[i, :len(selected_samples[i])] = batch_weights[i]\n",
    "                y[i] = selected_labels[i]\n",
    "            #If the news is very long\n",
    "            #Cut it to the max_news_len\n",
    "            else:\n",
    "                x[i, :] = selected_samples[i][:max_len]\n",
    "                w[i, :] = batch_weights[i][:max_len]\n",
    "                y[i] = selected_labels[i]\n",
    "        return x, y, lengths, w\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration, it is clear that the length of the news varies much, ranging from 10 to 10000. In order to deal with that case, we can take buckets into consideration, similar to seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 1\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(train_news_vecs)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(test_news_vecs)/trainConfig.batch_size)\n",
    "remain_num = len(test_news_labels) - trainConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_samples = generateSamples(train_news_vecs, train_news_labels, train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_samples = generateSamples(test_news_vecs, test_news_labels, test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, lengths, w = train_samples.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, lengths, w = test_samples.generate_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "In this model, we first transform each news as a series of word vectors. Then we put the series of news into a RNN system to get the final state vectors. Next, we do classification based on the news vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reference:http://blog.csdn.net/u010223750/article/details/71079036\n",
    "from tensorflow.contrib import rnn\n",
    "class CNN_Model:\n",
    "    def __init__(self, config, x, y, lengths, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.label_size = config.label_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lengths = lengths\n",
    "        self.max_doc_len = config.max_doc_len\n",
    "        self.is_training = is_training\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.inference\n",
    "        targets = tf.one_hot(self.y, 20, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.inference\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def accuracy(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        return accuracy\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "            #self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.0005).minimize(cost)\n",
    "            #train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "    @lazy_property\n",
    "    def inference(self):\n",
    "        #Create embedding matrix\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        if self.is_training:\n",
    "            inputs = tf.nn.dropout(inputs, 0.5)\n",
    "        #Expand the dim to cater to CNN\n",
    "        intputs_expanded = tf.expand_dims(inputs, -1)\n",
    "        with tf.variable_scope('CNN1'):\n",
    "            filter_shape = [3, self.embed_size, 1, 8]\n",
    "            W1 = tf.get_variable('W1', shape=filter_shape)\n",
    "            b1 = tf.get_variable('b1', shape=[8])\n",
    "            #798*1*8\n",
    "            conv1 = tf.nn.conv2d(intputs_expanded, W1, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv1\")\n",
    "            # Apply nonlinearity\n",
    "            h1 = tf.nn.relu(tf.nn.bias_add(conv1, b1), name=\"relu1\")\n",
    "            #shape=batch, 398, 1, 8\n",
    "            pooled1 = tf.nn.max_pool(h1, ksize=[1, 4, 1, 1],\n",
    "                                     strides=[1, 2, 1, 1],\n",
    "                                     padding='VALID',\n",
    "                                     name=\"pool1\")\n",
    "\n",
    "        with tf.variable_scope('CNN2'):\n",
    "            filter_shape = [3, 1, 8, 8]\n",
    "            W2 = tf.get_variable('W2', shape=filter_shape)\n",
    "            b2 = tf.get_variable('b2', shape=[8])\n",
    "            #shape=batch, 396, 1, 8\n",
    "            conv2 = tf.nn.conv2d(pooled1, W2, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv2\")\n",
    "            # Apply nonlinearity\n",
    "            h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n",
    "            #shape=batch, 197, 1, 8\n",
    "            pooled2 = tf.nn.max_pool(h2, ksize=[1, 4, 1, 1],\n",
    "                                     strides=[1, 2, 1, 1],\n",
    "                                     padding='VALID',\n",
    "                                     name=\"POOL2\")\n",
    "\n",
    "            #Fully Connected Layer\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            W_fc1 = tf.get_variable('W_fc1', shape=[197*8, 128])#weight_variable([7 * 7 * 64, 1024])\n",
    "            b_fc1 = tf.get_variable('b_fc1', shape=[128])#bias_variable([1024])\n",
    "            h_pool2_flat = tf.reshape(pooled2, [-1, 197*8])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "            #Dropout, to prevent against overfitting\n",
    "            #h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('output'):\n",
    "            weights = tf.get_variable('weights', [128, self.label_size], dtype=tf.float32)\n",
    "            biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(h_fc1, weights) + biases\n",
    "        #预测值\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        train_data = tf.placeholder(tf.int32, [trainConfig.batch_size, None])\n",
    "        train_label = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        train_weight = tf.placeholder(tf.float32, [trainConfig.batch_size, None])\n",
    "        train_lengths = tf.placeholder(tf.float32, [trainConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model(trainConfig, train_data, train_label, train_lengths)\n",
    "    with tf.name_scope('test'):\n",
    "        test_data = tf.placeholder(tf.int32, [testConfig.batch_size, None])\n",
    "        test_label = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        test_weight = tf.placeholder(tf.float32, [testConfig.batch_size, None])\n",
    "        test_lengths = tf.placeholder(tf.float32, [testConfig.batch_size])\n",
    "        single_data = tf.placeholder(tf.int32, [singleConfig.batch_size, None])\n",
    "        single_label = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        single_weight = tf.placeholder(tf.float32, [singleConfig.batch_size, None])\n",
    "        single_lengths = tf.placeholder(tf.float32, [singleConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model(testConfig, test_data, test_label, test_lengths, False)\n",
    "            single_model = CNN_Model(singleConfig, single_data, single_label, single_lengths, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.6875"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news_vecs)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news_vecs) - 64*117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.9959 Accuracy: 0.094\n",
      "Loss: 2.9656 Accuracy: 0.062\n",
      "Epoch 0 time:11.11\n",
      "Loss: 2.4277 Accuracy: 0.141\n",
      "Loss: 2.1634 Accuracy: 0.219\n",
      "Epoch 1 time:11.10\n",
      "Loss: 2.1547 Accuracy: 0.109\n",
      "Loss: 1.731 Accuracy: 0.297\n",
      "Epoch 2 time:11.00\n",
      "Loss: 1.5872 Accuracy: 0.469\n",
      "Loss: 1.9427 Accuracy: 0.438\n",
      "Epoch 3 time:11.07\n",
      "Loss: 1.4907 Accuracy: 0.359\n",
      "Loss: 1.4578 Accuracy: 0.406\n",
      "Epoch 4 time:11.06\n",
      "Loss: 1.3905 Accuracy: 0.484\n",
      "Loss: 1.2998 Accuracy: 0.594\n",
      "Epoch 5 time:11.06\n",
      "Loss: 1.139 Accuracy: 0.484\n",
      "Loss: 1.0314 Accuracy: 0.578\n",
      "Epoch 6 time:11.05\n",
      "Loss: 1.2717 Accuracy: 0.594\n",
      "Loss: 1.0393 Accuracy: 0.609\n",
      "Epoch 7 time:11.13\n",
      "Loss: 0.8287 Accuracy: 0.797\n",
      "Loss: 0.8785 Accuracy: 0.688\n",
      "Epoch 8 time:11.12\n",
      "Loss: 0.837 Accuracy: 0.719\n",
      "Loss: 0.9559 Accuracy: 0.75\n",
      "Epoch 9 time:11.09\n",
      "Loss: 0.7941 Accuracy: 0.734\n",
      "Loss: 0.7599 Accuracy: 0.703\n",
      "Epoch 10 time:11.04\n",
      "Loss: 0.7663 Accuracy: 0.75\n",
      "Loss: 0.7257 Accuracy: 0.734\n",
      "Epoch 11 time:11.10\n",
      "Loss: 0.6414 Accuracy: 0.828\n",
      "Loss: 0.5011 Accuracy: 0.812\n",
      "Epoch 12 time:11.13\n",
      "Loss: 0.5959 Accuracy: 0.828\n",
      "Loss: 0.4112 Accuracy: 0.859\n",
      "Epoch 13 time:11.44\n",
      "Loss: 0.503 Accuracy: 0.844\n",
      "Loss: 0.4682 Accuracy: 0.797\n",
      "Epoch 14 time:11.29\n",
      "Loss: 0.461 Accuracy: 0.812\n",
      "Loss: 0.4553 Accuracy: 0.797\n",
      "Epoch 15 time:11.14\n",
      "Loss: 0.3841 Accuracy: 0.891\n",
      "Loss: 0.6013 Accuracy: 0.828\n",
      "Epoch 16 time:11.18\n",
      "Loss: 0.2848 Accuracy: 0.906\n",
      "Loss: 0.5742 Accuracy: 0.828\n",
      "Epoch 17 time:11.10\n",
      "Loss: 0.2007 Accuracy: 0.938\n",
      "Loss: 0.2909 Accuracy: 0.891\n",
      "Epoch 18 time:11.47\n",
      "Loss: 0.2431 Accuracy: 0.922\n",
      "Loss: 0.299 Accuracy: 0.922\n",
      "Epoch 19 time:11.40\n",
      "Loss: 0.2952 Accuracy: 0.906\n",
      "Loss: 0.1478 Accuracy: 0.938\n",
      "Epoch 20 time:11.17\n",
      "Loss: 0.2342 Accuracy: 0.938\n",
      "Loss: 0.2659 Accuracy: 0.938\n",
      "Epoch 21 time:11.22\n",
      "Loss: 0.383 Accuracy: 0.922\n",
      "Loss: 0.2988 Accuracy: 0.875\n",
      "Epoch 22 time:11.30\n",
      "Loss: 0.2291 Accuracy: 0.922\n",
      "Loss: 0.2826 Accuracy: 0.922\n",
      "Epoch 23 time:11.44\n",
      "Loss: 0.1837 Accuracy: 0.938\n",
      "Loss: 0.2334 Accuracy: 0.953\n",
      "Epoch 24 time:11.21\n",
      "Loss: 0.2138 Accuracy: 0.906\n",
      "Loss: 0.2375 Accuracy: 0.891\n",
      "Epoch 25 time:11.35\n",
      "Loss: 0.3086 Accuracy: 0.891\n",
      "Loss: 0.0987 Accuracy: 0.953\n",
      "Epoch 26 time:11.43\n",
      "Loss: 0.1251 Accuracy: 0.953\n",
      "Loss: 0.2711 Accuracy: 0.906\n",
      "Epoch 27 time:11.24\n",
      "Loss: 0.4061 Accuracy: 0.828\n",
      "Loss: 0.1554 Accuracy: 0.953\n",
      "Epoch 28 time:11.35\n",
      "Loss: 0.2215 Accuracy: 0.922\n",
      "Loss: 0.1494 Accuracy: 0.938\n",
      "Epoch 29 time:11.32\n",
      "Testing...\n",
      "Test Samples come to an end!\n",
      "Testing Time:1.60\n",
      "0.345193839618\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 30\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths, w = train_samples.generate_batch()\n",
    "            feed_dict = {train_data:x, train_label:y, train_lengths:lengths}\n",
    "            l, a, _ = sess.run([train_model.cost, train_model.accuracy, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4), 'Accuracy:', round(a, 3))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        start_time = end_time\n",
    "    #Calculate Testing Accuracy\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    test_samples = generateSamples(test_news_vecs, test_news_labels, test_weights)\n",
    "    for _ in range(117):\n",
    "        #Traverse each data\n",
    "        x, y, lengths, w = test_samples.generate_batch(64, False)\n",
    "        feed_dict = {test_data:x, test_label:y, test_lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(44):\n",
    "        #Traverse each data\n",
    "        x, y, lengths, w = test_samples.generate_batch(1, False)\n",
    "        feed_dict = {single_data:x, single_label:y, single_lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_news_vecs))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems overfitting happens for convolutional neural networks."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

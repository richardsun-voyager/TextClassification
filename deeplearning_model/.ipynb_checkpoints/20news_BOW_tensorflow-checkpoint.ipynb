{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                  shuffle=True, random_state=11)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                  shuffle=True, random_state=11)\n",
    "#newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "#                                  shuffle=True, random_state=11)\n",
    "#newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
    "#                                  shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text number: 11314\n",
      "Testing text number: 7532\n"
     ]
    }
   ],
   "source": [
    "print('Training text number:', len(newsgroups_train.data))\n",
    "print('Testing text number:', len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class readNews:\n",
    "    '''\n",
    "    Read 20news and transform them into vectors for training\n",
    "    Args:\n",
    "    train_data\n",
    "    test_data\n",
    "    '''\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._preprocess()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('['+string.punctuation+']', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[^a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        '''Remove punctuations'''\n",
    "        train_news = self._train_data.data\n",
    "        test_news = self._test_data.data\n",
    "        self._train_data.data = [self._preProcessor(item) for item in train_news]\n",
    "        self._test_data.data = [self._preProcessor(item) for item in test_news]\n",
    "        \n",
    "    def _tfidf_vectorizer(self):\n",
    "        ''''Vectorize news'''\n",
    "        tfidfVectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_features=5000)\n",
    "        X_train_tfidf = tfidfVectorizer.fit_transform(self._train_data.data)\n",
    "        X_test_tfidf = tfidfVectorizer.transform(self._test_data.data)\n",
    "        vocab_index_dict = tfidfVectorizer.vocabulary_\n",
    "        return X_train_tfidf, X_test_tfidf, vocab_index_dict\n",
    "    \n",
    "    def tfidf_weight(self):\n",
    "        '''Calculate TfIdf weights for each word within each news'''\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        X_train_tfidf, X_test_tfidf, vocab_index_dict = self._tfidf_vectorizer()\n",
    "        train_weights = []\n",
    "        test_weights = []\n",
    "        #Generate dicts for words and corresponding tfidf weights\n",
    "        for i, news in enumerate(train_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_train_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            train_weights.append(word_weight)\n",
    "        for i, news in enumerate(test_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_test_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            test_weights.append(word_weight)      \n",
    "        return train_weights, test_weights\n",
    "    \n",
    "    def _news2words(self):\n",
    "        #Split each news into words\n",
    "        train_news_words = []\n",
    "        test_news_words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect words for each news\n",
    "           train_news_words.append(news.split())\n",
    "        for news in self._test_data.data:\n",
    "            test_news_words.append(news.split())\n",
    "        return train_news_words, test_news_words\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect all the chars\n",
    "           words.extend(news.split())\n",
    "        #Calculate frequencies of each character\n",
    "        word_freq = Counter(words)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in word_freq.items() if v>3]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return vocab, word_id_map, id_word_map\n",
    "    \n",
    "    def news2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        vocab, word_id_map, id_word_mapp = self.buildVocab()\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        #Turn each news into a list of word Ids\n",
    "        words_vecs = lambda words: [word2id(w) for w in words]\n",
    "        train_news_vecs = [words_vecs(words) for words in train_news_words]\n",
    "        train_news_labels = self._train_data.target\n",
    "        test_news_vecs = [words_vecs(words) for words in test_news_words]\n",
    "        test_news_labels = self._test_data.target\n",
    "        return train_news_vecs, train_news_labels, test_news_vecs, test_news_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a readnews object\n",
    "rn = readNews(newsgroups_train, newsgroups_test)\n",
    "train_news_vecs, train_news_labels, test_news_vecs, test_news_labels = rn.news2vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Record tfidf weights for each word in each news\n",
    "train_weights, test_weights = rn.tfidf_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the vocabulary and dictionary of words as well as corresponding ids\n",
    "vocab, word_id_map, id_word_map = rn.buildVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(c):\n",
    "    try:\n",
    "        ID = word_id_map[c]\n",
    "    except:#Trun those less frequent words into UNK\n",
    "        ID = word_id_map['UNK']\n",
    "    return ID\n",
    "def id2word(c):\n",
    "    try:\n",
    "        word = id_word_map[c]\n",
    "    except:\n",
    "        word='UNK'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Length 17\n",
      "Max Length 15804\n",
      "Median Length 184.0\n"
     ]
    }
   ],
   "source": [
    "train_news_length = [len(news) for news in train_news_vecs]\n",
    "print('Min Length', np.amin(train_news_length))\n",
    "print('Max Length', np.max(train_news_length))\n",
    "print('Median Length', np.median(train_news_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 116.,  184.,  301.,  509.,  769.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_news_length, [25, 50, 75, 90, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the length varies much, perhaps we need buckets to put news with similar lengths together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort the news vectors by its length\n",
    "news_lens_pair = list(zip(train_news_length, train_news_vecs, train_news_labels))\n",
    "news_lens_pair.sort()\n",
    "train_news_vecs_sorted = [item[1] for item in news_lens_pair]\n",
    "train_news_labels_sorted = [item[2] for item in news_lens_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwSFJ6mJwSJK6GBySpC4GhySpy/8DmCKRYPHy7S8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1223f27f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(train_news_length, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_news_length = [len(item) for item in test_news_vecs]\n",
    "#Sort the testing news vectors by its length\n",
    "news_lens_pair = list(zip(test_news_length, test_news_vecs, test_news_labels))\n",
    "news_lens_pair.sort()\n",
    "test_news_vecs_sorted = [item[1] for item in news_lens_pair]\n",
    "test_news_labels_sorted = [item[2] for item in news_lens_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_news_length_sorted = [len(item) for item in train_news_vecs_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter those very long or very short texts\n",
    "#Filter out those very long or very short news\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "for i, news in enumerate(train_news_vecs_sorted):\n",
    "    if len(news) > 10 and len(news)  < 400:\n",
    "        train_data.append(news)\n",
    "        train_label.append(train_news_labels_sorted[i])\n",
    "for i, news in enumerate(test_news_vecs_sorted):\n",
    "    if len(news)  > 10 and len(news)  < 400:\n",
    "        test_data.append(news)\n",
    "        test_label.append(test_news_labels_sorted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x122046278>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVkRJwczMyvi5GBmZkWcHMzMrMj/B/XVqHQjCqjBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1223f2358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The lengths of news\n",
    "plt.scatter(range(len(train_news_length_sorted)), train_news_length_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bucket Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class generateSamples:\n",
    "    '''Generate samples for training and testing'''\n",
    "    \n",
    "    def __init__(self, batch_size, news_vecs, news_labels, weights, max_len=1000):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.news_vecs = news_vecs\n",
    "        self.news_labels = news_labels\n",
    "        self.weights= weights\n",
    "        self.news_count = len(news_vecs)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def generate_batch(self, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        start = self.index%self.news_count\n",
    "        end = (start + self.batch_size)%self.news_count\n",
    "        #In case end goes beyong the range of the samples\n",
    "        if end > start:\n",
    "            data = self.news_vecs[start: end]\n",
    "            label = self.news_labels[start: end]\n",
    "            #record weights\n",
    "            batch_weight = self.weights[start:end]\n",
    "            self.index = end\n",
    "        else:\n",
    "            if is_training:#Continue to make new samples\n",
    "                self.index = end\n",
    "                data = self.news_vecs[start: ] + self.news_vecs[: end]\n",
    "                #record weights\n",
    "                batch_weight = self.weights[start:] + self.weights[: end]\n",
    "                #record labels\n",
    "                label = np.zeros(self.batch_size)\n",
    "                label[end:] = self.news_labels[start: ] \n",
    "                label[:end] = self.news_labels[:end]\n",
    "            else:#End making new samples\n",
    "                print('Test Samples come to an end!')\n",
    "                data = self.news_vecs[start: ]\n",
    "                #record weights\n",
    "                batch_weight = self.weights[start:]\n",
    "                label = self.news_labels[start: ]\n",
    "                self.index = 0\n",
    "        #Set the max lengths as the size of the input\n",
    "        #max_len = max(map(len, data))\n",
    "        max_len = self.max_len\n",
    "        #Record lengths for each text\n",
    "        lengths = [len(item) for item in data]\n",
    "        lengths = np.array(lengths)\n",
    "\n",
    "        sample_num = len(data)\n",
    "        #Create input and label\n",
    "        x = np.full((sample_num,max_len), word2id('UNK'), np.int32)\n",
    "        w = np.full((sample_num,max_len), 0, np.float32)\n",
    "        y = np.zeros(sample_num)\n",
    "        for i in range(sample_num):\n",
    "            #the first n elements as input\n",
    "            x[i, :len(data[i])] = data[i]\n",
    "            w[i, :len(data[i])] = batch_weight[i]\n",
    "            y[i] = label[i]\n",
    "        return x, y, lengths, w\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration, it is clear that the length of the news varies much, ranging from 10 to 10000. In order to deal with that case, we can take buckets into consideration, similar to seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set several buckets in advance\n",
    "buckets = [120, 190, 305, 510, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data2buckets(news_vecs, news_labels, news_weights):\n",
    "    bucket_data = [[] for _ in range(len(buckets))]\n",
    "    for i, vec in enumerate(news_vecs):\n",
    "        label = news_labels[i]\n",
    "        w = news_weights[i]\n",
    "        #If the news is too long, cut the tail\n",
    "        #And put it into the last bucket\n",
    "        if len(vec) > buckets[-1]:\n",
    "            vec = vec[:buckets[-1]]\n",
    "            w = w[:buckets[-1]]\n",
    "            #bucket_data[-1].append((vec, label, w))\n",
    "        #Otherwise, put it into the other bucket\n",
    "        else:\n",
    "            for l, b in enumerate(buckets):\n",
    "                if len(vec) <= b:\n",
    "                    bucket_data[l].append((vec, label, w))\n",
    "                    break\n",
    "    return bucket_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pts in bucket 0: 3044\n",
      "Data pts in bucket 1: 2866\n",
      "Data pts in bucket 2: 2651\n",
      "Data pts in bucket 3: 1630\n",
      "Data pts in bucket 4: 589\n"
     ]
    }
   ],
   "source": [
    "#Separate original data into buckets\n",
    "bucket_data = data2buckets(train_news_vecs, train_news_labels, train_weights)\n",
    "bucket_data_test = data2buckets(test_news_vecs, test_news_labels, test_weights)\n",
    "# Print summaries of buckets\n",
    "train_bucket_sizes = [len(bucket_data[b]) for b in range(len(buckets))]\n",
    "train_total_size = float(sum(train_bucket_sizes))\n",
    "for ix, bucket in enumerate(bucket_data):\n",
    "    print('Data pts in bucket {}: {}'.format(ix, len(bucket)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class generateBucketSamples:\n",
    "    '''Generate samples for training'''\n",
    "    \n",
    "    def __init__(self, batch_size, bucket_data, buckets):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.bucket_data = bucket_data\n",
    "        self.buckets = buckets\n",
    "        \n",
    "        \n",
    "    def generate_batch(self, bucket_id, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        \n",
    "        #The size of the specified bucket\n",
    "        bucket_len = self.buckets[bucket_id]\n",
    "        bucket_sample = []\n",
    "        for _ in range(self.batch_size):\n",
    "            if is_training:\n",
    "                vec, label, w = random.choice(self.bucket_data[bucket_id])\n",
    "            else:\n",
    "                #During testing, fetch each sampleone by one\n",
    "                vec, label, w = self.bucket_data[bucket_id][self.index]\n",
    "                self.index += self.batch_size\n",
    "                if self.index > bucket_len:\n",
    "                    self.index = 0\n",
    "            ID = word_id_map['UNK']\n",
    "            #Pad the vec with 'unk' index\n",
    "            #To make the vec the same length\n",
    "            left = bucket_len - len(vec)\n",
    "            vec = vec + [ID] * left\n",
    "            w = w + [0] * left\n",
    "            bucket_sample.append((vec, label, w))\n",
    "        vecs, labels, weights = zip(*bucket_sample)\n",
    "        vecs = np.array(vecs, np.int32)\n",
    "        labels = np.array(labels, np.int32)\n",
    "        weights = np.array(weights, np.float32)\n",
    "        return vecs, labels, weights      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 1\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(train_news_vecs)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(test_news_vecs)/trainConfig.batch_size)\n",
    "remain_num = len(test_news_labels) - trainConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_samples = generateBucketSamples(trainConfig.batch_size, bucket_data, buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_samples = generateBucketSamples(testConfig.batch_size, bucket_data_test, buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word2Vec Model\n",
    "\n",
    "In this model, we first transform each news as a series of word vectors. Then we represent each news with the average vector for those word vectors. Next, we do classification based on the news vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BOW_Bucket_Model:\n",
    "    def __init__(self, config, x, y, weight, bucket_size, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.label_size = config.label_size\n",
    "        self.max_doc_len = config.max_doc_len\n",
    "        self.word_weights = weight\n",
    "        self.bucket_size = bucket_size\n",
    "        self.is_training = is_training\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.bow\n",
    "        targets = tf.one_hot(self.y, 20, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.bow\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "            self._learning_rate = tf.get_variable(name='learning_rate',  \n",
    "                                                  shape=(), trainable=False)\n",
    "            #train_op = tf.train.AdamOptimizer(0.0005).minimize(cost)\n",
    "            train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "    @lazy_property\n",
    "    def bow(self):\n",
    "        #Create embedding matrix\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        if self.is_training:\n",
    "            inputs = tf.nn.dropout(inputs, 0.5)\n",
    "        #Take word tfidf weights into consideration\n",
    "        w = tf.expand_dims(self.word_weights, -1)\n",
    "        #Each vector is multiplied by a weight extracted by tfidf value\n",
    "        inputs = inputs * w\n",
    "        #slice inputs into a series of vectors\n",
    "        batch_doc_vectors = tf.unstack(inputs, axis=0)\n",
    "        vecs = []\n",
    "        #Consider the real length of each news\n",
    "        #Calculate the mean vector for each article, ignore the padding words\n",
    "        for i, doc_vector in enumerate(batch_doc_vectors):\n",
    "            doc = tf.reduce_mean(doc_vector[:self.bucket_size,:], axis=0)\n",
    "            vecs.append(doc)\n",
    "        mean_vector = tf.reshape(vecs, [len(vecs), self.embed_size])\n",
    "        weights = tf.get_variable('weights', [self.embed_size, self.label_size], dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(mean_vector, weights) + biases\n",
    "        #预测值\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_bow = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_bow.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        train_data = tf.placeholder(tf.int32, [trainConfig.batch_size, None])\n",
    "        train_label = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        train_weight = tf.placeholder(tf.float32, [trainConfig.batch_size, None])\n",
    "        train_models = [[] for _ in range(len(buckets))]\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", initializer=initializer):\n",
    "            for i in range(len(buckets)):\n",
    "                if i > 0: tf.get_variable_scope().reuse_variables()\n",
    "                train_models[i] = BOW_Bucket_Model(trainConfig, train_data, train_label, train_weight, buckets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "with graph_bow.as_default():\n",
    "    #Set models which share parameters with training models\n",
    "    #Use different models of different buckets\n",
    "    with tf.name_scope('test'):\n",
    "        test_data = tf.placeholder(tf.int32, [testConfig.batch_size, None])\n",
    "        test_label = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        test_weight = tf.placeholder(tf.float32, [testConfig.batch_size, None])\n",
    "        test_models = [[] for _ in range(len(buckets))]\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            for i in range(len(buckets)):\n",
    "                test_models[i] = BOW_Bucket_Model(testConfig, test_data, test_label, test_weight, buckets[i], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.99484\n",
      "Loss: 2.98977\n",
      "Loss: 2.99197\n",
      "Loss: 2.99344\n",
      "Loss: 2.9925\n",
      "Loss: 2.97531\n",
      "Loss: 2.97994\n",
      "Loss: 2.96955\n",
      "Loss: 2.9608\n",
      "Loss: 2.9487\n",
      "Loss: 2.94583\n",
      "Loss: 2.93366\n",
      "Loss: 2.92975\n",
      "Loss: 2.91671\n",
      "Loss: 2.90103\n",
      "Loss: 2.87174\n",
      "Loss: 2.89\n",
      "Loss: 2.86994\n",
      "Loss: 2.85236\n",
      "Loss: 2.80844\n",
      "Loss: 2.81384\n",
      "Loss: 2.76015\n",
      "Loss: 2.76916\n",
      "Loss: 2.80547\n",
      "Loss: 2.76218\n",
      "Loss: 2.73069\n",
      "Loss: 2.70168\n",
      "Loss: 2.62434\n",
      "Loss: 2.67584\n",
      "Loss: 2.63645\n",
      "Loss: 2.63347\n",
      "Loss: 2.61627\n",
      "Loss: 2.54821\n",
      "Loss: 2.48422\n",
      "Loss: 2.49989\n",
      "Loss: 2.50473\n",
      "Loss: 2.47598\n",
      "Loss: 2.54388\n",
      "Loss: 2.36498\n",
      "Loss: 2.43195\n",
      "Loss: 2.36338\n",
      "Loss: 2.47713\n",
      "Loss: 2.46507\n",
      "Loss: 2.40212\n",
      "Loss: 2.26394\n",
      "Loss: 2.28197\n",
      "Loss: 2.30139\n",
      "Loss: 2.27344\n",
      "Loss: 2.30803\n",
      "Loss: 2.2299\n",
      "Loss: 2.3945\n",
      "Loss: 2.21597\n",
      "Loss: 2.19549\n",
      "Loss: 2.32018\n",
      "Loss: 2.03451\n",
      "Loss: 2.22019\n",
      "Loss: 1.94456\n",
      "Loss: 2.05289\n",
      "Loss: 2.12689\n",
      "Loss: 1.8862\n",
      "Loss: 1.90861\n",
      "Loss: 2.00542\n",
      "Loss: 1.97895\n",
      "Loss: 2.05833\n",
      "Loss: 1.83861\n",
      "Loss: 1.92523\n",
      "Loss: 1.76683\n",
      "Loss: 1.8371\n",
      "Loss: 1.80465\n",
      "Loss: 1.94949\n",
      "Loss: 1.72875\n",
      "Loss: 1.77735\n",
      "Loss: 1.73137\n",
      "Loss: 1.77602\n",
      "Loss: 1.72073\n",
      "Loss: 1.72281\n",
      "Loss: 1.6436\n",
      "Loss: 1.77946\n",
      "Loss: 1.74391\n",
      "Loss: 1.68014\n",
      "Loss: 1.59769\n",
      "Loss: 1.83635\n",
      "Loss: 1.53505\n",
      "Loss: 1.79906\n",
      "Loss: 1.49391\n",
      "Loss: 1.54046\n",
      "Loss: 1.60599\n",
      "Loss: 1.62929\n",
      "Loss: 1.58013\n",
      "Loss: 1.5528\n",
      "Loss: 1.57163\n",
      "Loss: 1.4801\n",
      "Loss: 1.51289\n",
      "Loss: 1.43948\n",
      "Loss: 1.50853\n",
      "Loss: 1.37292\n",
      "Loss: 1.38131\n",
      "Loss: 1.48525\n",
      "Loss: 1.46384\n",
      "Loss: 1.34926\n",
      "Loss: 1.49011\n",
      "Loss: 1.33246\n",
      "Loss: 1.52037\n",
      "Loss: 1.32871\n",
      "Loss: 1.3489\n",
      "Loss: 1.26544\n",
      "Loss: 1.55691\n",
      "Loss: 1.47753\n",
      "Loss: 1.35761\n",
      "Loss: 1.31569\n",
      "Loss: 1.29969\n",
      "Loss: 1.38709\n",
      "Loss: 1.4889\n",
      "Loss: 1.67741\n",
      "Loss: 1.38636\n",
      "Loss: 1.15887\n",
      "Loss: 1.2395\n",
      "Loss: 1.32378\n",
      "Loss: 1.26017\n",
      "Loss: 1.43145\n",
      "Loss: 1.16301\n",
      "Loss: 1.3074\n",
      "Loss: 1.20914\n",
      "Loss: 1.2138\n",
      "Loss: 1.35698\n",
      "Loss: 1.38241\n",
      "Loss: 1.37468\n",
      "Loss: 1.30295\n",
      "Loss: 1.09196\n",
      "Loss: 1.09084\n",
      "Loss: 1.13115\n",
      "Loss: 1.07707\n",
      "Loss: 1.37863\n",
      "Loss: 1.35529\n",
      "Loss: 1.10411\n",
      "Loss: 1.29775\n",
      "Loss: 1.07636\n",
      "Loss: 1.26265\n",
      "Loss: 1.34853\n",
      "Loss: 1.1926\n",
      "Loss: 1.22304\n",
      "Loss: 1.20756\n",
      "Loss: 1.17288\n",
      "Loss: 1.02205\n",
      "Loss: 1.33564\n",
      "Loss: 0.960876\n",
      "Loss: 1.02979\n",
      "Loss: 1.1646\n",
      "Loss: 1.05742\n",
      "Loss: 1.16088\n",
      "Loss: 1.05924\n",
      "Loss: 1.08624\n",
      "Loss: 0.893249\n",
      "Loss: 0.8938\n",
      "Loss: 0.967081\n",
      "Loss: 0.99754\n",
      "Loss: 1.01014\n",
      "Loss: 1.1759\n",
      "Loss: 0.89406\n",
      "Loss: 1.0379\n",
      "Loss: 1.07892\n",
      "Loss: 1.08505\n",
      "Loss: 1.14554\n",
      "Loss: 1.00109\n",
      "Loss: 1.03846\n",
      "Loss: 1.02291\n",
      "Loss: 0.890014\n",
      "Loss: 0.901547\n",
      "Loss: 0.879991\n",
      "Loss: 0.97605\n",
      "Loss: 1.07884\n",
      "Loss: 0.871197\n",
      "Loss: 0.99655\n",
      "Loss: 0.974015\n",
      "Loss: 0.992943\n",
      "Loss: 1.15126\n",
      "Loss: 1.13087\n",
      "Loss: 1.05909\n",
      "Loss: 1.1165\n",
      "Loss: 1.06833\n",
      "Loss: 0.998328\n",
      "Loss: 1.1921\n",
      "Loss: 0.986476\n",
      "Loss: 0.939644\n",
      "Loss: 1.04496\n",
      "Loss: 0.976356\n",
      "Loss: 0.788927\n",
      "Loss: 1.0147\n",
      "Loss: 1.00696\n",
      "Loss: 0.947517\n",
      "Loss: 1.02151\n",
      "Loss: 0.925035\n",
      "Loss: 0.780985\n",
      "Loss: 0.906834\n",
      "Loss: 1.00756\n",
      "Loss: 0.995342\n",
      "Loss: 0.852647\n",
      "Loss: 0.696738\n",
      "Loss: 0.82299\n",
      "Loss: 0.973249\n",
      "0.670472650027\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "with tf.Session(graph=graph_bow) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for l in range(epochs):\n",
    "        #Use exponential decaying learning rate\n",
    "        if l%3 == 0:\n",
    "            sess.run(tf.assign(train_models[0].learningRate, 0.0005*(0.98**l)))\n",
    "        for i in range(train_chunk_num):\n",
    "            #Assign a learning rate\n",
    "            #sess.run(tf.assign(model.isTraining, True))\n",
    "            bucket_id = np.random.choice(len(buckets))\n",
    "            vecs, labels, weights = train_samples.generate_batch(bucket_id)\n",
    "            feed_dict = {train_data:vecs, train_label:labels, train_weight:weights}\n",
    "            loss, _ = sess.run([train_models[bucket_id].cost, train_models[bucket_id].optimize], feed_dict=feed_dict)\n",
    "            if i% 200 == 0:\n",
    "                print('Loss:', loss)\n",
    "    #Traverse each bucket of testing data\n",
    "    print('*' * 20)\n",
    "    print('Testing Accuracy:')\n",
    "    count = 0\n",
    "    test_samples = generateBucketSamples(testConfig.batch_size, bucket_data_test, buckets)\n",
    "    for bucket_id in range(len(buckets)):\n",
    "        #Traverse each data within each bucket\n",
    "        for _ in bucket_data_test[bucket_id]:\n",
    "            vecs, labels, weights = test_samples.generate_batch(bucket_id)\n",
    "            feed_dict = {test_data:vecs, test_label:labels, test_weight:weights}\n",
    "            n = sess.run(test_models[bucket_id].correct_num, feed_dict=feed_dict)\n",
    "            count += np.sum(n)\n",
    "    print(count*1.0/len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

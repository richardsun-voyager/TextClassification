{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                  shuffle=True, random_state=11)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                  shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text number: 11314\n",
      "Testing text number: 7532\n"
     ]
    }
   ],
   "source": [
    "print('Training text number:', len(newsgroups_train.data))\n",
    "print('Testing text number:', len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class readNews:\n",
    "    '''\n",
    "    Read 20news and transform them into vectors for training\n",
    "    Args:\n",
    "    train_data\n",
    "    test_data\n",
    "    '''\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._preprocess()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('['+string.punctuation+']', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[^a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        '''Remove punctuations'''\n",
    "        train_news = self._train_data.data\n",
    "        test_news = self._test_data.data\n",
    "        self._train_data.data = [self._preProcessor(item) for item in train_news]\n",
    "        self._test_data.data = [self._preProcessor(item) for item in test_news]\n",
    "        \n",
    "    def _tfidf_vectorizer(self):\n",
    "        ''''Vectorize news'''\n",
    "        tfidfVectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_features=5000)\n",
    "        X_train_tfidf = tfidfVectorizer.fit_transform(self._train_data.data)\n",
    "        X_test_tfidf = tfidfVectorizer.transform(self._test_data.data)\n",
    "        vocab_index_dict = tfidfVectorizer.vocabulary_\n",
    "        return X_train_tfidf, X_test_tfidf, vocab_index_dict\n",
    "    \n",
    "    def tfidf_weight(self):\n",
    "        '''Calculate TfIdf weights for each word within each news'''\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        X_train_tfidf, X_test_tfidf, vocab_index_dict = self._tfidf_vectorizer()\n",
    "        train_weights = []\n",
    "        test_weights = []\n",
    "        #Generate dicts for words and corresponding tfidf weights\n",
    "        for i, news in enumerate(train_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_train_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            train_weights.append(word_weight)\n",
    "        for i, news in enumerate(test_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_test_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            test_weights.append(word_weight)      \n",
    "        return train_weights, test_weights\n",
    "    \n",
    "    def _news2words(self):\n",
    "        #Split each news into words\n",
    "        train_news_words = []\n",
    "        test_news_words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect words for each news\n",
    "           train_news_words.append(news.split())\n",
    "        for news in self._test_data.data:\n",
    "            test_news_words.append(news.split())\n",
    "        return train_news_words, test_news_words\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect all the chars\n",
    "           words.extend(news.split())\n",
    "        #Calculate frequencies of each character\n",
    "        word_freq = Counter(words)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in word_freq.items() if v>3]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return vocab, word_id_map, id_word_map\n",
    "    \n",
    "    def news2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        vocab, word_id_map, id_word_mapp = self.buildVocab()\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        #Turn each news into a list of word Ids\n",
    "        words_vecs = lambda words: [word2id(w) for w in words]\n",
    "        train_news_vecs = [words_vecs(words) for words in train_news_words]\n",
    "        train_news_labels = self._train_data.target\n",
    "        test_news_vecs = [words_vecs(words) for words in test_news_words]\n",
    "        test_news_labels = self._test_data.target\n",
    "        return train_news_vecs, train_news_labels, test_news_vecs, test_news_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a readnews object\n",
    "rn = readNews(newsgroups_train, newsgroups_test)\n",
    "train_news_vecs, train_news_labels, test_news_vecs, test_news_labels = rn.news2vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Record tfidf weights for each word in each news\n",
    "#train_weights, test_weights = rn.tfidf_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the vocabulary and dictionary of words as well as corresponding ids\n",
    "vocab, word_id_map, id_word_map = rn.buildVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(c):\n",
    "    try:\n",
    "        ID = word_id_map[c]\n",
    "    except:#Trun those less frequent words into UNK\n",
    "        ID = word_id_map['UNK']\n",
    "    return ID\n",
    "def id2word(c):\n",
    "    try:\n",
    "        word = id_word_map[c]\n",
    "    except:\n",
    "        word='UNK'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Length 17\n",
      "Max Length 15804\n",
      "Median Length 184.0\n"
     ]
    }
   ],
   "source": [
    "train_news_length = [len(news) for news in train_news_vecs]\n",
    "print('Min Length', np.amin(train_news_length))\n",
    "print('Max Length', np.max(train_news_length))\n",
    "print('Median Length', np.median(train_news_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17.,  116.,  184.,  301.,  509.,  769.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_news_length, [0, 25, 50, 75, 90, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the length varies much, perhaps we need buckets to put news with similar lengths together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class generateSamples:\n",
    "    '''Generate samples for training and testing'''\n",
    "    \n",
    "    def __init__(self, news_vecs, news_labels, max_len=800):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.index = 0\n",
    "        self.news_vecs = news_vecs\n",
    "        self.news_labels = news_labels\n",
    "        self.news_count = len(news_vecs)\n",
    "        self.max_news_len = max_len\n",
    "        \n",
    "    def generate_batch(self, batch_size=64, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        \n",
    "        selected_samples = []\n",
    "        selected_labels = []\n",
    "        #For training, select random samples\n",
    "        if is_training:\n",
    "            selected_index = np.random.choice(len(self.news_vecs), batch_size, replace=True)\n",
    "            for index in selected_index:\n",
    "                selected_samples.append(self.news_vecs[index])\n",
    "                selected_labels.append(self.news_labels[index])\n",
    "        #For testing, select a few samples each time\n",
    "        else:#Testing model\n",
    "            start = self.index%self.news_count\n",
    "            end = (start + batch_size)%self.news_count\n",
    "            #In case end goes beyong the range of the samples\n",
    "            if end > start:\n",
    "                selected_samples = self.news_vecs[start: end]\n",
    "                selected_labels = self.news_labels[start: end]\n",
    "                self.index = end\n",
    "            else:\n",
    "                print('Test Samples come to an end!')\n",
    "                selected_samples = self.news_vecs[start: ]\n",
    "                selected_labels = self.news_labels[start: ]\n",
    "                self.index = 0\n",
    "            \n",
    "        #Set the max lengths as the size of the input\n",
    "        #max_len = max(map(len, data))\n",
    "        #Record lengths for each text\n",
    "        lengths = [len(item) for item in selected_samples]\n",
    "        lengths = np.array(lengths)\n",
    "        #Get the max length in current batch\n",
    "        max_len = max(lengths)\n",
    "        max_len = self.max_news_len if max_len > self.max_news_len else max_len\n",
    "\n",
    "        #Create input and label\n",
    "        x = np.full((batch_size, max_len), word2id('UNK'), np.int32)\n",
    "        y = np.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            #the first n elements as input\n",
    "            if len(selected_samples[i]) < max_len:\n",
    "                x[i, :len(selected_samples[i])] = selected_samples[i]\n",
    "                y[i] = selected_labels[i]\n",
    "            #If the news is very long\n",
    "            #Cut it to the max_news_len\n",
    "            else:\n",
    "                x[i, :] = selected_samples[i][:max_len]\n",
    "                y[i] = selected_labels[i]\n",
    "        return x, y, lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration, it is clear that the length of the news varies much, ranging from 10 to 10000. In order to deal with that case, we can take buckets into consideration, similar to seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 128\n",
    "    batch_size = 1\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(train_news_vecs)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(test_news_vecs)/trainConfig.batch_size)\n",
    "remain_num = len(test_news_labels) - trainConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_samples = generateSamples(train_news_vecs, train_news_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_samples = generateSamples(test_news_vecs, test_news_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, lengths = train_samples.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, lengths = test_samples.generate_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic RNN Model\n",
    "\n",
    "In this model, we first transform each news as a series of word vectors. Then we put the series of news into a RNN system to get the final state vectors. Next, we do classification based on the news vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reference:http://blog.csdn.net/u010223750/article/details/71079036\n",
    "from tensorflow.contrib import rnn\n",
    "class BiRNN_Model:\n",
    "    def __init__(self, config, x, y, lengths, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.label_size = config.label_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lengths = lengths\n",
    "        self.max_doc_len = config.max_doc_len\n",
    "        self.is_training = is_training\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.inference\n",
    "        targets = tf.one_hot(self.y, 20, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.inference\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "            #self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.0005).minimize(cost)\n",
    "            #train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "    @lazy_property\n",
    "    def inference(self):\n",
    "        #Create embedding matrix\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        if self.is_training:\n",
    "            inputs = tf.nn.dropout(inputs, 0.5)\n",
    "\n",
    "        #slice inputs into a series of vectors\n",
    "        #batch_doc_vectors = tf.unstack(inputs, axis=0)\n",
    "        def lstm():\n",
    "            return rnn.BasicLSTMCell(self.embed_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True) \n",
    "        #lstm_cell = lstm\n",
    "        #cell = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], \n",
    "                                #state_is_tuple=True)\n",
    "        lstm_fw_cell = lstm()\n",
    "        lstm_bw_cell = lstm()\n",
    "        initial_fw_state = lstm_fw_cell.zero_state(self.batch_size, tf.float32)\n",
    "        initial_bw_state = lstm_bw_cell.zero_state(self.batch_size, tf.float32)\n",
    "        #Bidirectional dynamic RNN with given lengths for each text\n",
    "        outputs, status = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, inputs,\n",
    "                                                          initial_state_fw=initial_fw_state,\n",
    "                                                          initial_state_bw=initial_bw_state,\n",
    "                                                          sequence_length=self.lengths, dtype=tf.float32)\n",
    "        #output = outputs[:,-1,:]\n",
    "        #If we use padding, the last output will be based on the padding input values\n",
    "        #Note here we use the hidden state instead of the last output\n",
    "        #In dynamic rnn, the last state will remain the same after specified time steps\n",
    "        #For example, if the length is 10 and the padding sequence has 20 words, the \n",
    "        #final state will be the one of 10th time step\n",
    "        #COncatenate the two hidden states\n",
    "        #print(status[1].h)\n",
    "        output = tf.concat([status[0].h, status[1].h], axis=1)\n",
    "        \n",
    "        \n",
    "        weights = tf.get_variable('weights', [2*self.embed_size, self.label_size], dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(output, weights) + biases\n",
    "        #预测值\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_birnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_birnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        train_data = tf.placeholder(tf.int32, [trainConfig.batch_size, None])\n",
    "        train_label = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        train_lengths = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = BiRNN_Model(trainConfig, train_data, train_label, train_lengths)\n",
    "    with tf.name_scope('test'):\n",
    "        test_data = tf.placeholder(tf.int32, [testConfig.batch_size, None])\n",
    "        test_label = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        test_lengths = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        single_data = tf.placeholder(tf.int32, [singleConfig.batch_size, None])\n",
    "        single_label = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        single_lengths = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = BiRNN_Model(testConfig, test_data, test_label, test_lengths, False)\n",
    "            single_model = BiRNN_Model(singleConfig, single_data, single_label, single_lengths, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.6875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news_vecs)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news_vecs) - 64*117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.995\n",
      "Loss: 2.979\n",
      "Epoch 0 time:172.94\n",
      "Loss: 2.8216\n",
      "Loss: 2.5011\n",
      "Epoch 1 time:173.14\n",
      "Loss: 2.4381\n",
      "Loss: 2.2444\n",
      "Epoch 2 time:172.67\n",
      "Loss: 1.8115\n",
      "Loss: 2.0834\n",
      "Epoch 3 time:171.38\n",
      "Loss: 1.827\n",
      "Loss: 1.4616\n",
      "Epoch 4 time:171.21\n",
      "Loss: 1.3666\n",
      "Loss: 2.3901\n",
      "Epoch 5 time:173.48\n",
      "Loss: 1.1999\n",
      "Loss: 1.1597\n",
      "Epoch 6 time:173.18\n",
      "Loss: 1.1915\n",
      "Loss: 0.8363\n",
      "Epoch 7 time:173.63\n",
      "Loss: 0.7313\n",
      "Loss: 0.919\n",
      "Epoch 8 time:172.54\n",
      "Loss: 0.9635\n",
      "Loss: 0.7441\n",
      "Epoch 9 time:171.12\n",
      "Loss: 0.8278\n",
      "Loss: 0.6081\n",
      "Epoch 10 time:172.59\n",
      "Loss: 0.6202\n",
      "Loss: 0.4946\n",
      "Epoch 11 time:173.95\n",
      "Loss: 0.3739\n",
      "Loss: 0.7374\n",
      "Epoch 12 time:171.25\n",
      "Loss: 0.3572\n",
      "Loss: 0.3614\n",
      "Epoch 13 time:173.87\n",
      "Loss: 0.2641\n",
      "Loss: 0.2091\n",
      "Epoch 14 time:172.71\n",
      "Loss: 0.3236\n",
      "Loss: 0.3306\n",
      "Epoch 15 time:174.83\n",
      "Loss: 0.2707\n",
      "Loss: 0.3751\n",
      "Epoch 16 time:171.60\n",
      "Loss: 0.2333\n",
      "Loss: 0.3598\n",
      "Epoch 17 time:171.86\n",
      "Loss: 0.2254\n",
      "Loss: 0.2952\n",
      "Epoch 18 time:170.41\n",
      "Loss: 0.1887\n",
      "Loss: 0.1345\n",
      "Epoch 19 time:170.75\n",
      "Loss: 0.1172\n",
      "Loss: 0.1832\n",
      "Epoch 20 time:173.30\n",
      "Loss: 0.2014\n",
      "Loss: 0.3127\n",
      "Epoch 21 time:172.86\n",
      "Loss: 0.1347\n",
      "Loss: 0.2613\n",
      "Epoch 22 time:170.43\n",
      "Loss: 0.0948\n",
      "Loss: 0.1533\n",
      "Epoch 23 time:173.23\n",
      "Loss: 0.1431\n",
      "Loss: 0.0986\n",
      "Epoch 24 time:173.28\n",
      "Loss: 0.1162\n",
      "Loss: 0.0761\n",
      "Epoch 25 time:170.36\n",
      "Loss: 0.0846\n",
      "Loss: 0.3027\n",
      "Epoch 26 time:173.17\n",
      "Loss: 0.1074\n",
      "Loss: 0.09\n",
      "Epoch 27 time:172.75\n",
      "Loss: 0.0696\n",
      "Loss: 0.1002\n",
      "Epoch 28 time:172.64\n",
      "Loss: 0.0741\n",
      "Loss: 0.0572\n",
      "Epoch 29 time:171.78\n",
      "Loss: 0.0267\n",
      "Loss: 0.3156\n",
      "Epoch 30 time:173.31\n",
      "Loss: 0.0969\n",
      "Loss: 0.0694\n",
      "Epoch 31 time:174.86\n",
      "Loss: 0.2019\n",
      "Loss: 0.054\n",
      "Epoch 32 time:172.98\n",
      "Loss: 0.0659\n",
      "Loss: 0.0543\n",
      "Epoch 33 time:172.26\n",
      "Loss: 0.0407\n",
      "Loss: 0.0989\n",
      "Epoch 34 time:171.94\n",
      "Loss: 0.1683\n",
      "Loss: 0.4345\n",
      "Epoch 35 time:172.97\n",
      "Loss: 0.19\n",
      "Loss: 0.1125\n",
      "Epoch 36 time:172.92\n",
      "Loss: 0.0427\n",
      "Loss: 0.0701\n",
      "Epoch 37 time:172.50\n",
      "Loss: 0.0694\n",
      "Loss: 0.1029\n",
      "Epoch 38 time:172.45\n",
      "Loss: 0.0438\n",
      "Loss: 0.027\n",
      "Epoch 39 time:173.01\n",
      "Loss: 0.0554\n",
      "Loss: 0.0958\n",
      "Epoch 40 time:171.81\n",
      "Loss: 0.1716\n",
      "Loss: 0.0151\n",
      "Epoch 41 time:35592.53\n",
      "Loss: 0.1029\n",
      "Loss: 0.0281\n",
      "Epoch 42 time:174.88\n",
      "Loss: 0.0171\n",
      "Loss: 0.0627\n",
      "Epoch 43 time:175.26\n",
      "Loss: 0.1667\n",
      "Loss: 0.008\n",
      "Epoch 44 time:173.96\n",
      "Loss: 0.2428\n",
      "Loss: 0.031\n",
      "Epoch 45 time:173.64\n",
      "Loss: 0.0223\n",
      "Loss: 0.0558\n",
      "Epoch 46 time:180.00\n",
      "Loss: 0.0319\n",
      "Loss: 0.0131\n",
      "Epoch 47 time:179.67\n",
      "Loss: 0.0526\n",
      "Loss: 0.057\n",
      "Epoch 48 time:174.38\n",
      "Loss: 0.0282\n",
      "Loss: 0.0097\n",
      "Epoch 49 time:175.90\n",
      "Testing...\n",
      "Test Samples come to an end!\n",
      "Testing Time:39.85\n",
      "0.552044609665\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 50\n",
    "with tf.Session(graph=graph_birnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_samples.generate_batch()\n",
    "            feed_dict = {train_data:x, train_label:y, train_lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        start_time = end_time\n",
    "    #Calculate Testing Accuracy\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    test_samples = generateSamples(test_news_vecs, test_news_labels)\n",
    "    for _ in range(117):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_samples.generate_batch(64, False)\n",
    "        feed_dict = {test_data:x, test_label:y, test_lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(44):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_samples.generate_batch(1, False)\n",
    "        feed_dict = {single_data:x, single_label:y, single_lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_news_vecs))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It works much better than one-direction RNN."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

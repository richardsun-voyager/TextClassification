{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to classify texts through an attention model, for more information, please visit [Hierarchical Attention Networks for Document Classification](http://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                  shuffle=True, random_state=11)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                  shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text number: 11314\n",
      "Testing text number: 7532\n"
     ]
    }
   ],
   "source": [
    "print('Training text number:', len(newsgroups_train.data))\n",
    "print('Testing text number:', len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class readNews:\n",
    "    '''\n",
    "    Read 20news and transform them into vectors for training\n",
    "    Args:\n",
    "    train_data\n",
    "    test_data\n",
    "    '''\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._preprocess()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('['+string.punctuation+']', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[^a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        '''Remove punctuations'''\n",
    "        train_news = self._train_data.data\n",
    "        test_news = self._test_data.data\n",
    "        self._train_data.data = [self._preProcessor(item) for item in train_news]\n",
    "        self._test_data.data = [self._preProcessor(item) for item in test_news]\n",
    "        \n",
    "    def _tfidf_vectorizer(self):\n",
    "        ''''Vectorize news'''\n",
    "        tfidfVectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_features=5000)\n",
    "        X_train_tfidf = tfidfVectorizer.fit_transform(self._train_data.data)\n",
    "        X_test_tfidf = tfidfVectorizer.transform(self._test_data.data)\n",
    "        vocab_index_dict = tfidfVectorizer.vocabulary_\n",
    "        return X_train_tfidf, X_test_tfidf, vocab_index_dict\n",
    "    \n",
    "    def tfidf_weight(self):\n",
    "        '''Calculate TfIdf weights for each word within each news'''\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        X_train_tfidf, X_test_tfidf, vocab_index_dict = self._tfidf_vectorizer()\n",
    "        train_weights = []\n",
    "        test_weights = []\n",
    "        #Generate dicts for words and corresponding tfidf weights\n",
    "        for i, news in enumerate(train_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_train_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            train_weights.append(word_weight)\n",
    "        for i, news in enumerate(test_news_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_test_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            test_weights.append(word_weight)      \n",
    "        return train_weights, test_weights\n",
    "    \n",
    "    def _news2words(self):\n",
    "        #Split each news into words\n",
    "        train_news_words = []\n",
    "        test_news_words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect words for each news\n",
    "           train_news_words.append(news.split())\n",
    "        for news in self._test_data.data:\n",
    "            test_news_words.append(news.split())\n",
    "        return train_news_words, test_news_words\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        words = []\n",
    "        for news in self._train_data.data:\n",
    "           #Collect all the chars\n",
    "           words.extend(news.split())\n",
    "        #Calculate frequencies of each character\n",
    "        word_freq = Counter(words)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in word_freq.items() if v>3]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return vocab, word_id_map, id_word_map\n",
    "    \n",
    "    def news2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        train_news_words, test_news_words = self._news2words()\n",
    "        vocab, word_id_map, id_word_mapp = self.buildVocab()\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        #Turn each news into a list of word Ids\n",
    "        words_vecs = lambda words: [word2id(w) for w in words]\n",
    "        train_news_vecs = [words_vecs(words) for words in train_news_words]\n",
    "        train_news_labels = self._train_data.target\n",
    "        test_news_vecs = [words_vecs(words) for words in test_news_words]\n",
    "        test_news_labels = self._test_data.target\n",
    "        return train_news_vecs, train_news_labels, test_news_vecs, test_news_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a readnews object\n",
    "rn = readNews(newsgroups_train, newsgroups_test)\n",
    "train_news_vecs, train_news_labels, test_news_vecs, test_news_labels = rn.news2vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Record tfidf weights for each word in each news\n",
    "#train_weights, test_weights = rn.tfidf_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the vocabulary and dictionary of words as well as corresponding ids\n",
    "vocab, word_id_map, id_word_map = rn.buildVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(c):\n",
    "    try:\n",
    "        ID = word_id_map[c]\n",
    "    except:#Trun those less frequent words into UNK\n",
    "        ID = word_id_map['UNK']\n",
    "    return ID\n",
    "def id2word(c):\n",
    "    try:\n",
    "        word = id_word_map[c]\n",
    "    except:\n",
    "        word='UNK'\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Length 17\n",
      "Max Length 15804\n",
      "Median Length 184.0\n"
     ]
    }
   ],
   "source": [
    "train_news_length = [len(news) for news in train_news_vecs]\n",
    "print('Min Length', np.amin(train_news_length))\n",
    "print('Max Length', np.max(train_news_length))\n",
    "print('Median Length', np.median(train_news_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17.,  116.,  184.,  301.,  509.,  769.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_news_length, [0, 25, 50, 75, 90, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the length varies much, perhaps we need buckets to put news with similar lengths together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class generateSamples:\n",
    "    '''Generate samples for training and testing'''\n",
    "    \n",
    "    def __init__(self, news_vecs, news_labels, max_len=800):\n",
    "        '''Pass batch size and poems vectors'''\n",
    "        self.index = 0\n",
    "        self.news_vecs = news_vecs\n",
    "        self.news_labels = news_labels\n",
    "        self.news_count = len(news_vecs)\n",
    "        self.max_news_len = max_len\n",
    "        \n",
    "    def generate_batch(self, batch_size=32, is_training=True):\n",
    "        '''Generate a training sample each time'''\n",
    "        \n",
    "        selected_samples = []\n",
    "        selected_labels = []\n",
    "        #For training, select random samples\n",
    "        if is_training:\n",
    "            selected_index = np.random.choice(len(self.news_vecs), batch_size, replace=True)\n",
    "            for index in selected_index:\n",
    "                selected_samples.append(self.news_vecs[index])\n",
    "                selected_labels.append(self.news_labels[index])\n",
    "        #For testing, select a few samples each time\n",
    "        else:#Testing model\n",
    "            start = self.index%self.news_count\n",
    "            end = (start + batch_size)%self.news_count\n",
    "            #In case end goes beyong the range of the samples\n",
    "            if end > start:\n",
    "                selected_samples = self.news_vecs[start: end]\n",
    "                selected_labels = self.news_labels[start: end]\n",
    "                self.index = end\n",
    "            else:\n",
    "                print('Test Samples come to an end!')\n",
    "                selected_samples = self.news_vecs[start: ]\n",
    "                selected_labels = self.news_labels[start: ]\n",
    "                self.index = 0\n",
    "            \n",
    "        #Set the max lengths as the size of the input\n",
    "        #max_len = max(map(len, data))\n",
    "        #Record lengths for each text\n",
    "        lengths = [len(item) for item in selected_samples]\n",
    "        lengths = np.array(lengths)\n",
    "        #Get the max length in current batch\n",
    "        max_len = max(lengths)\n",
    "        max_len = self.max_news_len if max_len > self.max_news_len else max_len\n",
    "\n",
    "        #Create input and label\n",
    "        x = np.full((batch_size, max_len), word2id('UNK'), np.int32)\n",
    "        y = np.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            #the first n elements as input\n",
    "            if len(selected_samples[i]) < max_len:\n",
    "                x[i, :len(selected_samples[i])] = selected_samples[i]\n",
    "                y[i] = selected_labels[i]\n",
    "            #If the news is very long\n",
    "            #Cut it to the max_news_len\n",
    "            else:\n",
    "                x[i, :] = selected_samples[i][:max_len]\n",
    "                y[i] = selected_labels[i]\n",
    "        return x, y, lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration, it is clear that the length of the news varies much, ranging from 10 to 10000. In order to deal with that case, we can take buckets into consideration, similar to seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trainConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 64\n",
    "    hidden_size = 50\n",
    "    batch_size = 32\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 64\n",
    "    hidden_size = 50\n",
    "    batch_size = 32\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    vocab_size = len(vocab)\n",
    "    max_doc_len = max(map(len, train_news_vecs))\n",
    "    label_size = 20\n",
    "    embed_size = 64\n",
    "    hidden_size = 50#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num = int(len(train_news_vecs)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(test_news_vecs)/trainConfig.batch_size)\n",
    "remain_num = len(test_news_labels) - trainConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_samples = generateSamples(train_news_vecs, train_news_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_samples = generateSamples(test_news_vecs, test_news_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, lengths = train_samples.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, lengths = test_samples.generate_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic RNN Model\n",
    "\n",
    "In this model, we first transform each news as a series of word vectors. Then we put the series of news into a RNN system to get the final state vectors. Next, we do classification based on the news vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "class HiRNN_Model:\n",
    "    def __init__(self, config, x, y, lengths, is_training=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_size = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.label_size = config.label_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lengths = lengths\n",
    "        self.max_doc_len = config.max_doc_len\n",
    "        self.is_training = is_training\n",
    "        self.predict\n",
    "        if is_training:\n",
    "            self.optimize\n",
    "        print('Model Initialized!')\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        logits = self.inference\n",
    "        targets = tf.one_hot(self.y, 20, 1, 0)\n",
    "        targets = tf.cast(targets, tf.float32)\n",
    "        #Note  tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=activation)\n",
    "        loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "        return loss\n",
    "    \n",
    "    @lazy_property\n",
    "    def predict(self):\n",
    "        logits = self.inference\n",
    "        #probs = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        return predictions\n",
    "    \n",
    "    @lazy_property\n",
    "    def correct_num(self):\n",
    "        prediction = self.predict\n",
    "        targets = tf.reshape(self.y, [-1])\n",
    "        targets = tf.cast(targets, tf.int64)\n",
    "        correct_prediction = tf.equal(prediction, targets)\n",
    "        correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        return correct_num\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            cost = self.cost\n",
    "        #with tf.name_scope('Optimizer'):\n",
    "            #self._learning_rate = tf.Variable(0.0, trainable=False)\n",
    "            train_op = tf.train.AdamOptimizer(0.0002).minimize(cost)\n",
    "            #train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 6)\n",
    "            #optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "            #train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return train_op\n",
    "    \n",
    "    @lazy_property\n",
    "    def inference(self):\n",
    "        #Create embedding matrix\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embeddings = tf.get_variable('embedding', [self.vocab_size,  self.embed_size])\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.x)\n",
    "        if self.is_training:\n",
    "            inputs = tf.nn.dropout(inputs, 0.5)\n",
    "\n",
    "        def lstm():\n",
    "            return rnn.BasicLSTMCell(self.hidden_size, forget_bias=0.0, \n",
    "                                      state_is_tuple=True) \n",
    "        \n",
    "        def GRU():\n",
    "            return rnn.GRUCell(self.hidden_size)\n",
    "        #lstm_cell = lstm\n",
    "        #cell = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], \n",
    "                                #state_is_tuple=True)\n",
    "        fw_cell = GRU()\n",
    "        bw_cell = GRU()\n",
    "        initial_fw_state = fw_cell.zero_state(self.batch_size, tf.float32)\n",
    "        initial_bw_state = bw_cell.zero_state(self.batch_size, tf.float32)\n",
    "        #Bidirectional dynamic RNN with given lengths for each text\n",
    "        outputs, status = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, inputs,\n",
    "                                                          initial_state_fw=initial_fw_state,\n",
    "                                                          initial_state_bw=initial_bw_state,\n",
    "                                                          sequence_length=self.lengths, dtype=tf.float32)\n",
    "        #Use bidirectional rnn output as hidden states for words\n",
    "        #size=batch_size, word_length, word_embedding_size*2\n",
    "        H = tf.concat([outputs[0], outputs[1]], axis=2)\n",
    "        \n",
    "        #Calculate attention weights for each word\n",
    "        W_att = tf.get_variable('word_attention_weights', [self.hidden_size*2, 64])\n",
    "        b_att = tf.get_variable('word_attention_biases', [64])\n",
    "        W_u = tf.get_variable('attention_softmax_weights', [64, 1])\n",
    "        S = []\n",
    "        for i in np.arange(self.batch_size):\n",
    "            #Calculate the coefficients of attention\n",
    "            h = H[i, :, :]\n",
    "            u = tf.tanh(tf.matmul(h, W_att) + b_att)\n",
    "            #Softmax\n",
    "            A = tf.nn.softmax(tf.matmul(u, W_u))\n",
    "            #Transform original representation into a sum of weighted hidden states\n",
    "            s = tf.reduce_sum(A * h, 0)\n",
    "            S.append(s)\n",
    "        \n",
    "        #Put all the elements within the list into a tensor\n",
    "        S = tf.stack(S)\n",
    "        \n",
    "        weights = tf.get_variable('weights', [2*self.hidden_size, self.label_size], dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases', [self.label_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(S, weights) + biases\n",
    "        #预测值\n",
    "        return logits\n",
    "    \n",
    "    @property\n",
    "    def learningRate(self):\n",
    "        return self._learning_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "graph_hirnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_hirnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        train_data = tf.placeholder(tf.int32, [trainConfig.batch_size, None])\n",
    "        train_label = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        train_lengths = tf.placeholder(tf.int32, [trainConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = HiRNN_Model(trainConfig, train_data, train_label, train_lengths)\n",
    "            saver=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        test_data = tf.placeholder(tf.int32, [testConfig.batch_size, None])\n",
    "        test_label = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        test_lengths = tf.placeholder(tf.int32, [testConfig.batch_size])\n",
    "        single_data = tf.placeholder(tf.int32, [singleConfig.batch_size, None])\n",
    "        single_label = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        single_lengths = tf.placeholder(tf.int32, [singleConfig.batch_size])\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = HiRNN_Model(testConfig, test_data, test_label, test_lengths, False)\n",
    "            single_model = HiRNN_Model(singleConfig, single_data, single_label, single_lengths, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.375"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news_vecs)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news_vecs) - 32*235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/hirnn.ckpt\n",
      "Loss: 0.7534\n",
      "Loss: 0.435\n",
      "Loss: 0.067\n",
      "Loss: 0.4061\n",
      "Epoch 0 time:531.69\n",
      "Loss: 0.2718\n",
      "Loss: 0.0653\n",
      "Loss: 0.1781\n",
      "Loss: 0.3721\n",
      "Epoch 1 time:521.69\n",
      "Loss: 0.0557\n",
      "Loss: 0.0224\n",
      "Loss: 0.12\n",
      "Loss: 0.1574\n",
      "Epoch 2 time:536.57\n",
      "Loss: 0.0461\n",
      "Loss: 0.1167\n",
      "Loss: 0.0401\n",
      "Loss: 0.2093\n",
      "Epoch 3 time:541.73\n",
      "Loss: 0.3925\n",
      "Loss: 0.044\n",
      "Loss: 0.0242\n",
      "Loss: 0.0283\n",
      "Epoch 4 time:538.77\n",
      "Loss: 0.0233\n",
      "Loss: 0.0447\n",
      "Loss: 0.0832\n",
      "Loss: 0.097\n",
      "Epoch 5 time:530.69\n",
      "Loss: 0.027\n",
      "Loss: 0.1539\n",
      "Loss: 0.019\n",
      "Loss: 0.0128\n",
      "Epoch 6 time:524.18\n",
      "Loss: 0.034\n",
      "Loss: 0.0106\n",
      "Loss: 1.0473\n",
      "Loss: 0.0093\n",
      "Epoch 7 time:541.98\n",
      "Loss: 0.0109\n",
      "Loss: 0.0597\n",
      "Loss: 0.0185\n",
      "Loss: 0.0225\n",
      "Epoch 8 time:557.64\n",
      "Loss: 0.0666\n",
      "Loss: 0.0059\n",
      "Loss: 0.5415\n",
      "Loss: 0.037\n",
      "Epoch 9 time:556.79\n",
      "Testing...\n",
      "Test Samples come to an end!\n",
      "Testing Time:95.27\n",
      "0.701938396176\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "epochs = 10\n",
    "file = \"ckpt/hirnn.ckpt\"\n",
    "with tf.Session(graph=graph_hirnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    if os.path.exists(\"ckpt/hirnn.ckpt.index\"):\n",
    "        saver.restore(sess, file)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_samples.generate_batch()\n",
    "            feed_dict = {train_data:x, train_label:y, train_lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        start_time = end_time\n",
    "    saver.save(sess,'ckpt/hirnn.ckpt')\n",
    "    #Calculate Testing Accuracy\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    test_samples = generateSamples(test_news_vecs, test_news_labels)\n",
    "    for _ in range(235):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_samples.generate_batch(32, False)\n",
    "        feed_dict = {test_data:x, test_label:y, test_lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(12):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_samples.generate_batch(1, False)\n",
    "        feed_dict = {single_data:x, single_label:y, single_lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_news_vecs))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It works pretty better than one-direction RNN. Next, we'll restore the final model and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/hirnn.ckpt\n",
      "Testing...\n",
      "Test Samples come to an end!\n",
      "Testing Time:698.66\n",
      "0.648433351036\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_hirnn) as sess:\n",
    "    saver.restore(sess, \"ckpt/hirnn.ckpt\")\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    test_samples = generateSamples(test_news_vecs, test_news_labels)\n",
    "    for _ in range(235):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_samples.generate_batch(32, False)\n",
    "        feed_dict = {test_data:x, test_label:y, test_lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(12):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_samples.generate_batch(1, False)\n",
    "        feed_dict = {single_data:x, single_label:y, single_lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_news_vecs))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

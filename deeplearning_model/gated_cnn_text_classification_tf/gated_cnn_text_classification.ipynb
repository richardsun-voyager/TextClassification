{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "max_document_length = max_document_length if max_document_length < 800 else 800\n",
    "#Cut long articles to 800 words. Pad short ones\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Restore the values from sparse matrix\n",
    "y_train = np.array([item.toarray().reshape(-1) for item in y_train])\n",
    "y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 2000\n",
    "    class_num = 20\n",
    "    embedding_size = 128\n",
    "    filter_size = 64\n",
    "    num_layers = 10\n",
    "    block_size = 5\n",
    "    filter_h = 5 #conv_kernel height_size\n",
    "    doc_len = 800#context_size\n",
    "    batch_size = 64\n",
    "    epochs = 50\n",
    "    num_sampled = 1\n",
    "    learning_rate = 1\n",
    "    momentum = 0.99\n",
    "    num_batches = 0\n",
    "    ckpt_path = \"cpkt\"\n",
    "    summary_path = \"logs\"\n",
    "    data_dir = \"data\"\n",
    "    grad_clip = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.vocab_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_conf(conf):\n",
    "    conf.filter_w = conf.embedding_size\n",
    "    #Pad the first k-1 item\n",
    "    conf.doc_len += int((conf.filter_h - 1)/2)\n",
    "    \n",
    "    # Check if data exists\n",
    "    if not os.path.exists(conf.data_dir):\n",
    "        exit(\"Please download the data as mentioned in Requirements\")\n",
    "\n",
    "    # Create paths for checkpointing\n",
    "    #ckpt_model_path = 'vocab%d_embed%d_filters%d_batch%d_layers%d_block%d_fdim%d'%(conf.vocab_size, conf.embedding_size, \n",
    "            #conf.filter_size, conf.batch_size, conf.num_layers, conf.block_size, conf.filter_h)\n",
    "    #conf.ckpt_path = os.path.join(conf.ckpt_path, ckpt_model_path)\n",
    "    conf.ckpt_path = 'cpkt'\n",
    "\n",
    "    if not os.path.exists(conf.ckpt_path):\n",
    "        os.makedirs(conf.ckpt_path)\n",
    "    conf.ckpt_file = os.path.join(conf.ckpt_path, \"model.ckpt\")\n",
    "\n",
    "    # Create Logs Folder\n",
    "    if tf.gfile.Exists(conf.summary_path):\n",
    "        tf.gfile.DeleteRecursively(conf.summary_path)\n",
    "    tf.gfile.MakeDirs(conf.summary_path)\n",
    "    return conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Created a Gated CNN model\n",
    "class GatedCNN(object):\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        ##tf.reset_default_graph()\n",
    "        #Input is a series of words\n",
    "        #Paddle the first beginning k-1 values as zeros\n",
    "        #doc_len = conf.doc_len + filter_h - 1\n",
    "        self.X = tf.placeholder(shape=[conf.batch_size, conf.doc_len], dtype=tf.int32, name=\"X\")\n",
    "        self.y = tf.placeholder(shape=[conf.batch_size], dtype=tf.int32, name=\"y\")\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            embed = self.create_embeddings(self.X, conf)\n",
    "            h, res_input = embed, embed\n",
    "        \n",
    "        with tf.name_scope(\"GatedConvLayers\"):\n",
    "            for i in range(conf.num_layers):\n",
    "                #Channels of current input\n",
    "                fanin_depth = h.get_shape()[-1]\n",
    "                #Set current filter size\n",
    "                filter_size = conf.filter_size if i < conf.num_layers-1 else 1\n",
    "                shape = (conf.filter_h, conf.filter_w, fanin_depth, filter_size)\n",
    "                with tf.variable_scope(\"layer_%d\"%i):\n",
    "                    conv_w = self.conv_op(h, shape, \"linear\")\n",
    "                    conv_v = self.conv_op(h, shape, \"gated\")\n",
    "                    h = conv_w * tf.sigmoid(conv_v)\n",
    "                    if i % conf.block_size == 0:\n",
    "                        h += res_input\n",
    "                        res_input = h\n",
    "            \n",
    "        #h = tf.reshape(h, (-1, conf.embedding_size))\n",
    "        h = tf.squeeze(h)\n",
    "        #Get the last one as the hidden state\n",
    "        h = h[:, -1, :]\n",
    "        print(h)\n",
    "        #Flatten\n",
    "        #h = tf.reshape(h, [conf.batch_size, -1])\n",
    "        h_final_size = h.get_shape()[-1]\n",
    "        y_shape = self.y.get_shape().as_list()\n",
    "        \n",
    "        #Fully connected layer\n",
    "        #with tf.variable_scope('Fully_Connected_Layer'):\n",
    "            #f_w = tf.get_variable(\"fully_w\", [h_final_size, conf.embedding_size], tf.float32, \n",
    "                                    #tf.random_normal_initializer(0.0, 0.1))\n",
    "            #f_b = tf.get_variable(\"fully_b\", [conf.embedding_size], tf.float32, \n",
    "                                    #tf.constant_initializer(1.0))\n",
    "            #h_fully = tf.matmul(h, f_w) + f_b\n",
    "\n",
    "        #self.y = tf.reshape(self.y, (y_shape[0] * y_shape[1], 1))\n",
    "        #Transform y into one-hot\n",
    "        #y = tf.one_hot(self.y, conf.class_num)\n",
    "        with tf.variable_scope('Output'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [conf.embedding_size, conf.class_num], tf.float32, \n",
    "                                    tf.random_normal_initializer(0.0, 0.1))\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [conf.class_num], tf.float32, \n",
    "                                    tf.constant_initializer(1.0))\n",
    "        \n",
    "            pred = tf.matmul(h, softmax_w) + softmax_b\n",
    "        #pred = tf.nn.softmax(pred)\n",
    "\n",
    "        #Preferance: NCE Loss, heirarchial softmax, adaptive softmax\n",
    "        #self.loss = tf.reduce_mean(tf.nn.nce_loss(softmax_w, softmax_b, h, self.y, \n",
    "                                                  #conf.num_sampled, conf.vocab_size))\n",
    "        self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=pred)\n",
    "\n",
    "        trainer = tf.train.MomentumOptimizer(conf.learning_rate, conf.momentum)\n",
    "        gradients = trainer.compute_gradients(self.loss)\n",
    "        clipped_gradients = [(tf.clip_by_value(_[0], -conf.grad_clip, conf.grad_clip), _[1]) for _ in gradients]\n",
    "        self.optimizer = trainer.apply_gradients(clipped_gradients)\n",
    "        #self.perplexity = tf.exp(self.loss)\n",
    "\n",
    "        self.create_summaries()\n",
    "\n",
    "    def create_embeddings(self, X, conf):\n",
    "\n",
    "        embeddings = tf.get_variable(\"embeds\",(conf.vocab_size, conf.embedding_size), tf.float32, \n",
    "                                     tf.random_uniform_initializer(-1.0,1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "        mask_layer = np.ones((conf.batch_size, conf.doc_len, conf.embedding_size))\n",
    "        #Zero Pad the first beginning k-1 values of the input\n",
    "        #In order to \n",
    "        #Batch Length Embedding\n",
    "        k = int(conf.filter_h/2)\n",
    "        mask_layer[:, :k, :] = 0\n",
    "        embed *= mask_layer\n",
    "        \n",
    "        embed_shape = embed.get_shape().as_list()\n",
    "        embed = tf.reshape(embed, (embed_shape[0], embed_shape[1], embed_shape[2], 1))\n",
    "        #expand_dim\n",
    "        return embed\n",
    "\n",
    "\n",
    "    def conv_op(self, fan_in, shape, name):\n",
    "        W = tf.get_variable(\"%s_W\"%name, shape, tf.float32, tf.random_normal_initializer(0.0, 0.1))\n",
    "        b = tf.get_variable(\"%s_b\"%name, shape[-1], tf.float32, tf.constant_initializer(1.0))\n",
    "        #Note the padding method is 'SAME', it will automatically pad the first k/2 items\n",
    "        return tf.add(tf.nn.conv2d(fan_in, W, strides=[1,1,1,1], padding='SAME'), b)\n",
    "    \n",
    "    def create_summaries(self):\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        #tf.summary.scalar(\"perplexity\", self.perplexity)\n",
    "        self.merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(64, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "conf = prepare_conf(conf)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gated_cnn = GatedCNN(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

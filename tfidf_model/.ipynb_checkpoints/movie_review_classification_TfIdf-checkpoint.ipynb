{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to classify 20newsgroup using bag-of-words model, and we represent each news as a vector of TfIdf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich i'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = movie_reviews.fileids()[0]\n",
    "movie_reviews.open(first).read()[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "积极评论数量： 1000\n",
      "负面评论数量： 1000\n"
     ]
    }
   ],
   "source": [
    "print('积极评论数量：', len(movie_reviews.fileids('pos')))\n",
    "print('负面评论数量：', len(movie_reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取每篇评论及其对应的标签\n",
    "document_label_pairs = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#随机打乱次序\n",
    "np.random.shuffle(document_label_pairs)\n",
    "documents, labels = list(zip(*document_label_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "doc_train, doc_test, label_train, label_test = train_test_split(documents, labels, test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Remove punctuation, lemmatize words and extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "punctuations = string.punctuation\n",
    "class textVectorizer:\n",
    "    '''\n",
    "    Clean texts and turn them into vectors\n",
    "    Args:\n",
    "    train_data: a list of texts(strings)\n",
    "    test_data: a list of texts(strings)\n",
    "    '''\n",
    "    def __init__(self, train_data, vectorizer, is_lemmatize=True):\n",
    "        self._train_data = train_data\n",
    "        self._vectorizer = vectorizer\n",
    "        self._is_lemmatize = is_lemmatize\n",
    "    \n",
    "    def proceed(self):\n",
    "        '''Execute preprocessing and intialize vectorizer'''\n",
    "        self._train_data = self._preprocess(self._train_data)\n",
    "        if self._is_lemmatize:\n",
    "            self._train_data = self._lemmatize(self._train_data)\n",
    "        X_train = self._vectorizer.fit_transform(self._train_data)\n",
    "        return X_train\n",
    "    \n",
    "    def vectorize(self, texts):\n",
    "        '''\n",
    "        Vectorize input texts\n",
    "        Args:\n",
    "        texts: list\n",
    "        '''\n",
    "        texts = self._preprocess(texts)\n",
    "        if self._is_lemmatize:\n",
    "            processed_vectors = self._lemmatize(texts)\n",
    "        vectors = self._vectorizer.transform(processed_vectors)\n",
    "        return vectors\n",
    "\n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('['+string.punctuation+']', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[^a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self, texts):\n",
    "        '''Remove punctuations'''\n",
    "        processed_texts = [self._preProcessor(item) for item in texts]\n",
    "        return processed_texts\n",
    "    \n",
    "    def _lemmatize(self, texts):\n",
    "        '''Lemmatize words into original forms'''\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        texts_lemmatized = []\n",
    "        for text in texts:\n",
    "            lem_data = [lemmatizer.lemmatize(word.strip()) for word in word_tokenize(text)]\n",
    "            data = ' '.join(lem_data)\n",
    "            texts_lemmatized.append(data)\n",
    "        return texts_lemmatized\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n",
    "selectKBest = SelectKBest(chi2, 10000)\n",
    "vectorizer1 = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "vectorizer2 = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "vectorizer3 = HashingVectorizer(ngram_range=(1, 2), non_negative=True, stop_words='english')\n",
    "vectorizer4 = CountVectorizer(stop_words='english')\n",
    "vectorizer5 = TfidfVectorizer(stop_words='english')\n",
    "vectorizer6 = HashingVectorizer(non_negative=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specify a classification model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4,\n",
    "                    solver='adam', verbose=False, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.17)\n",
    "nb = MultinomialNB(0.01)\n",
    "lr = LogisticRegression(C=0.5)\n",
    "rf =  RandomForestClassifier(n_estimators=100, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def train_test(clf, X_train=None, X_test=None):\n",
    "    start = time.time()\n",
    "    #训练\n",
    "    clf.fit(X_train, label_train)\n",
    "    end = time.time()\n",
    "    print('训练时间：{:.3f}'.format(end-start))\n",
    "    #预测\n",
    "    preds = clf.predict(X_train)\n",
    "    #计算准准确率\n",
    "    #Micro F1\n",
    "    print('Training accuracy: {:.3f}'.format(accuracy_score(label_train, preds)))\n",
    "    preds = clf.predict(X_test)\n",
    "    #计算准准确率\n",
    "    #Micro F1\n",
    "    print('Testing Accuracy: {:.3f}'.format(accuracy_score(label_test, preds)))\n",
    "    #print('Macro F1: {:.3f}'.format(f1_score(label_test, preds, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "训练时间：0.076\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.835\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "训练时间：0.416\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.795\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "训练时间：0.007\n",
      "Training accuracy: 0.996\n",
      "Testing Accuracy: 0.762\n",
      "******************************\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "训练时间：0.010\n",
      "Training accuracy: 0.939\n",
      "Testing Accuracy: 0.777\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "训练时间：0.416\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.807\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "训练时间：0.004\n",
      "Training accuracy: 0.996\n",
      "Testing Accuracy: 0.765\n",
      "******************************\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "训练时间：0.019\n",
      "Training accuracy: 0.854\n",
      "Testing Accuracy: 0.752\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "训练时间：0.432\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.798\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "训练时间：0.006\n",
      "Training accuracy: 0.994\n",
      "Testing Accuracy: 0.763\n",
      "******************************\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "训练时间：0.087\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.837\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "训练时间：0.417\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.780\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "训练时间：0.008\n",
      "Training accuracy: 0.986\n",
      "Testing Accuracy: 0.748\n",
      "******************************\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "训练时间：0.012\n",
      "Training accuracy: 0.942\n",
      "Testing Accuracy: 0.777\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "训练时间：0.420\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.780\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "训练时间：0.006\n",
      "Training accuracy: 0.992\n",
      "Testing Accuracy: 0.750\n",
      "******************************\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "训练时间：0.018\n",
      "Training accuracy: 0.870\n",
      "Testing Accuracy: 0.778\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "训练时间：0.420\n",
      "Training accuracy: 1.000\n",
      "Testing Accuracy: 0.802\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "训练时间：0.006\n",
      "Training accuracy: 0.978\n",
      "Testing Accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [vectorizer1, vectorizer2, vectorizer3, vectorizer4, vectorizer5, vectorizer6]\n",
    "#Train the model with several classifiers\n",
    "models = [lr, rf, nb]\n",
    "for vectorizer in vectorizers:\n",
    "    print('*'*30)\n",
    "    print(vectorizer.__class__)\n",
    "    tv = textVectorizer(doc_train, vectorizer)\n",
    "    X_train = tv.proceed()\n",
    "    X_test = tv.vectorize(doc_test)\n",
    "    X_train = selectKBest.fit_transform(X_train, label_train)\n",
    "    X_test = selectKBest.transform(X_test)\n",
    "    for model in models:      \n",
    "        print(model.__class__)\n",
    "        train_test(model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters\n",
    "\n",
    "We can select naive bayesian classifier as the final model, and with gridsearch we can figure out which 'alpha' can optimize the performance of the model on testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间：13.833\n",
      "Training accuracy: 0.960\n",
      "Testing Accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "parameters = {'C': [2, 1, 0.5, 0.2, 0.1, 0.05]}\n",
    "lr = LogisticRegression()\n",
    "gs = GridSearchCV(estimator=lr, param_grid=parameters, cv=5)\n",
    "train_test(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
